{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import vectorizers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/emilynaftalin/Data_Science/General Assembly/dsi/projects/project_3/practice_code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoic_zen = pd.read_csv('../datasets/clean_stoic_zen_tokenized.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>merged</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>you need to fight your mind every time it trie...</td>\n",
       "      <td>['you', 'need', 'to', 'fight', 'your', 'mind',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>a phone call creates stronger bonds than text ...</td>\n",
       "      <td>['a', 'phone', 'call', 'creates', 'stronger', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>everything in your life every experience, ever...</td>\n",
       "      <td>['everything', 'in', 'your', 'life', 'every', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>the parable of the mexican fisherman got me re...</td>\n",
       "      <td>['the', 'parable', 'of', 'the', 'mexican', 'fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>the key to success and productivity isn t to t...</td>\n",
       "      <td>['the', 'key', 'to', 'success', 'and', 'produc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                             merged  \\\n",
       "0      0  you need to fight your mind every time it trie...   \n",
       "1      0  a phone call creates stronger bonds than text ...   \n",
       "2      0  everything in your life every experience, ever...   \n",
       "3      0  the parable of the mexican fisherman got me re...   \n",
       "4      0  the key to success and productivity isn t to t...   \n",
       "\n",
       "                                              tokens  \n",
       "0  ['you', 'need', 'to', 'fight', 'your', 'mind',...  \n",
       "1  ['a', 'phone', 'call', 'creates', 'stronger', ...  \n",
       "2  ['everything', 'in', 'your', 'life', 'every', ...  \n",
       "3  ['the', 'parable', 'of', 'the', 'mexican', 'fi...  \n",
       "4  ['the', 'key', 'to', 'success', 'and', 'produc...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoic_zen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>merged</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5982</th>\n",
       "      <td>1</td>\n",
       "      <td>stoicism and self improvement i have just rece...</td>\n",
       "      <td>['stoicism', 'and', 'self', 'improvement', 'i'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5983</th>\n",
       "      <td>1</td>\n",
       "      <td>should i go cold turkey on entertainment to pr...</td>\n",
       "      <td>['should', 'i', 'go', 'cold', 'turkey', 'on', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5984</th>\n",
       "      <td>1</td>\n",
       "      <td>free law of attraction, the secret pdf and boo...</td>\n",
       "      <td>['free', 'law', 'of', 'attraction', 'the', 'se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5985</th>\n",
       "      <td>1</td>\n",
       "      <td>anxiety i have trouble rationalising with my a...</td>\n",
       "      <td>['anxiety', 'i', 'have', 'trouble', 'rationali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5986</th>\n",
       "      <td>1</td>\n",
       "      <td>\"some poor, phoneless fool is probably sitting...</td>\n",
       "      <td>['some', 'poor', 'phoneless', 'fool', 'is', 'p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                             merged  \\\n",
       "5982      1  stoicism and self improvement i have just rece...   \n",
       "5983      1  should i go cold turkey on entertainment to pr...   \n",
       "5984      1  free law of attraction, the secret pdf and boo...   \n",
       "5985      1  anxiety i have trouble rationalising with my a...   \n",
       "5986      1  \"some poor, phoneless fool is probably sitting...   \n",
       "\n",
       "                                                 tokens  \n",
       "5982  ['stoicism', 'and', 'self', 'improvement', 'i'...  \n",
       "5983  ['should', 'i', 'go', 'cold', 'turkey', 'on', ...  \n",
       "5984  ['free', 'law', 'of', 'attraction', 'the', 'se...  \n",
       "5985  ['anxiety', 'i', 'have', 'trouble', 'rationali...  \n",
       "5986  ['some', 'poor', 'phoneless', 'fool', 'is', 'p...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoic_zen.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NB: Making test size 0.25 in instead of 0.33 as it was in the KNN / Decision Tree Notebook_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stoic_zen['merged']\n",
    "y = stoic_zen['label']\n",
    "\n",
    "# splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                   y, \n",
    "                                                   test_size=0.25,\n",
    "                                                   shuffle=True,\n",
    "                                                   stratify=y,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`Random Forest` / `Extra Trees`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Random Forest + CountVectorizer` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming and Vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english')\n",
    "\n",
    "cvec.fit(X_train)\n",
    "\n",
    "X_train_cvec = cvec.transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Will start with a Random Forest Classifier using CountVectorized data and default hyperparameters._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=48)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=48)\n",
    "\n",
    "rf.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8601336302895323"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf, X_train_cvec, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8436722408026756"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(rf, X_test_cvec, y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest (default hyperparameters) score on training set: 0.999554565701559\n",
      "Random Forest (default hyperparameters) score on testing set: 0.8577154308617234\n"
     ]
    }
   ],
   "source": [
    "print(f'Random Forest (default hyperparameters) score on training set: {rf.score(X_train_cvec, y_train)}')\n",
    "print(f'Random Forest (default hyperparameters) score on testing set: {rf.score(X_test_cvec, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Extra Trees + CountVectorizer` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(random_state=48)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et = ExtraTreesClassifier(random_state=48)\n",
    "\n",
    "et.fit(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8623608017817371"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(et, X_train_cvec, y_train).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8450167224080267"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(et, X_test_cvec, y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees (default hyperparameters) score on training set: 0.999554565701559\n",
      "Extra Tress (default hyperparameters) score on testing set: 0.8643954575818303\n"
     ]
    }
   ],
   "source": [
    "print(f'Extra Trees (default hyperparameters) score on training set: {et.score(X_train_cvec, y_train)}')\n",
    "print(f'Extra Tress (default hyperparameters) score on testing set: {et.score(X_test_cvec, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Random + CountVectorizer Pipe` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Both Random Forest and Extra Trees Classifiers are **severely** overfit; next will move to a Pipeline and will tune a few hyperparameters._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "    ('rf', RandomForestClassifier(max_features=0.6, max_depth=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8171525760969353"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(pipe, X_train, y_train, cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(ngram_range=(1, 2), stop_words='english')),\n",
       "                ('rf', RandomForestClassifier(max_depth=5, max_features=0.6))])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8356347438752784"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8156312625250501"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Less overfit!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Extra Trees + CountVectorizer Pipeline` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_et = Pipeline([\n",
    "    ('cvec', CountVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "    ('et', ExtraTreesClassifier(max_features=0.6, max_depth=5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cvec',\n",
       "                 CountVectorizer(ngram_range=(1, 2), stop_words='english')),\n",
       "                ('et', ExtraTreesClassifier(max_depth=5, max_features=0.6))])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_et.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856792873051225"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_et.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8356713426853707"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_et.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Extra Trees + TfidfVectorizer Pipeline` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(ngram_range=(1, 2), stop_words='english')),\n",
       "                ('et', ExtraTreesClassifier(max_depth=5, max_features=0.6))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_et_tf = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english', ngram_range=(1,2))),\n",
    "    ('et', ExtraTreesClassifier(max_features=0.6, max_depth=5))\n",
    "])\n",
    "\n",
    "pipe_et_tf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.834966592427617"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_et_tf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8176352705410822"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_et_tf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Extra Trees has better accuracy than the Random Forest Pipeline but is still overfit._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Extra Trees + TfidfVectorizer Pipeline --> Gridsearch` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_To save time, will start with a gridsearch using Extra Trees since the accuracy was higher on the above pipeline._\n",
    "\n",
    "_I will countvectorizer as my transformer because this had higher accuracy than tfidf._\n",
    "\n",
    "_Hyperparameters are also informed by the best parameters found in the Decision Tree grid search in another notebook._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building cvec pipeline in two stages \n",
    "pipe_rf_cvec = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('et', ExtraTreesClassifier())\n",
    "])\n",
    "\n",
    "# hyperparamters for transformer and estimator \n",
    "pipe_params_rf_cvec = {\n",
    "    'cvec__max_features':[2000, 3000, 4000],\n",
    "    'cvec__ngram_range': [(1,2),(1,3)],\n",
    "    'cvec__min_df': [0.75, 1],\n",
    "    'et__ccp_alpha': [0.001, 0.1, 0, 1],\n",
    "    'et__max_depth': [4,6,8],\n",
    "    'et__min_samples_leaf': [4,6,8],\n",
    "    'et__min_samples_split': [5,10]\n",
    "}\n",
    "\n",
    "# gridsearch instantiation \n",
    "grid_rf_cvec = GridSearchCV(pipe_rf_cvec,\n",
    "                            pipe_params_rf_cvec,\n",
    "                            cv = 5,\n",
    "                            verbose = 2, \n",
    "                            n_jobs = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'dt__ccp_alpha': 0.001,\n",
    "#  'dt__max_depth': 8,\n",
    "#  'dt__min_samples_leaf': 6,\n",
    "#  'dt__min_samples_split': 5,\n",
    "#  'tfidf__max_features': 3000,\n",
    "#  'tfidf__min_df': 1,\n",
    "#  'tfidf__ngram_range': (1, 3)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 864 candidates, totalling 4320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/emilynaftalin/opt/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.75723831 0.76146993 0.76057906 0.75434298 0.7596882  0.7623608\n",
      " 0.77282851 0.77037862 0.77193764 0.77238307 0.76859688 0.76837416\n",
      " 0.78062361 0.77973274 0.78062361 0.78040089 0.77951002 0.77995546\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.75567929 0.76013363 0.75679287 0.75902004 0.75389755 0.76347439\n",
      " 0.7714922  0.77394209 0.77126949 0.77193764 0.77216036 0.76971047\n",
      " 0.78173719 0.78396437 0.78062361 0.77327394 0.77639198 0.78017817\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.76124722 0.76325167 0.76169265 0.75991091 0.75991091 0.76146993\n",
      " 0.7714922  0.76592428 0.76971047 0.76859688 0.77082405 0.77438753\n",
      " 0.77750557 0.78017817 0.77928731 0.77928731 0.78418708 0.78106904\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.7596882  0.76057906 0.75746102 0.75857461 0.75902004 0.75902004\n",
      " 0.77327394 0.76904232 0.77216036 0.77037862 0.76636971 0.77082405\n",
      " 0.7766147  0.77928731 0.77884187 0.77928731 0.77772829 0.77995546\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.76436526 0.75612472 0.75879733 0.75723831 0.7545657  0.75835189\n",
      " 0.76503341 0.7674833  0.76659243 0.76703786 0.76703786 0.76904232\n",
      " 0.77884187 0.7766147  0.77728285 0.78062361 0.77683742 0.77906459\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.75612472 0.75991091 0.7532294  0.75523385 0.75545657 0.75501114\n",
      " 0.76525612 0.76458797 0.7701559  0.76636971 0.77037862 0.76592428\n",
      " 0.77995546 0.77594655 0.77282851 0.77483296 0.7766147  0.77639198\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.75857461 0.75924276 0.75612472 0.75634744 0.7545657  0.7532294\n",
      " 0.76859688 0.76391982 0.76614699 0.76859688 0.76258352 0.7701559\n",
      " 0.77772829 0.77550111 0.77906459 0.7752784  0.77728285 0.77483296\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.75857461 0.75657016 0.75812918 0.74877506 0.76035635 0.75144766\n",
      " 0.76926503 0.76770601 0.76948775 0.77193764 0.7674833  0.76614699\n",
      " 0.77750557 0.77861915 0.77839644 0.77438753 0.77817372 0.77260579\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.75612472 0.75278396 0.75367483 0.74922049 0.75256125 0.75478842\n",
      " 0.76993318 0.76525612 0.76080178 0.76525612 0.76458797 0.76414254\n",
      " 0.77594655 0.7752784  0.77550111 0.77126949 0.77394209 0.77438753\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.75367483 0.75835189 0.75857461 0.7545657  0.75478842 0.7596882\n",
      " 0.76481069 0.76035635 0.76636971 0.76525612 0.76258352 0.76703786\n",
      " 0.7701559  0.77438753 0.77483296 0.77282851 0.77594655 0.77104677\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.75300668 0.75478842 0.75144766 0.75857461 0.75345212 0.75501114\n",
      " 0.77060134 0.76347439 0.76124722 0.76191537 0.76436526 0.76057906\n",
      " 0.77505568 0.77327394 0.7714922  0.77594655 0.77104677 0.77884187\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.75567929 0.75679287 0.74966592 0.75434298 0.7610245  0.7545657\n",
      " 0.76213808 0.76971047 0.76948775 0.76703786 0.76124722 0.7623608\n",
      " 0.77594655 0.77616927 0.77438753 0.77216036 0.7714922  0.777951\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272\n",
      " 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272 0.50022272]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This cell took 2968.1651360988617 seconds to run\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# gridsearching on training data\n",
    "grid_rf_cvec.fit(X_train, y_train)\n",
    "\n",
    "print(f'This cell took {time.time() - t0} seconds to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7841870824053453"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_cvec.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cvec__max_features': 2000,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 3),\n",
       " 'et__ccp_alpha': 0.001,\n",
       " 'et__max_depth': 8,\n",
       " 'et__min_samples_leaf': 8,\n",
       " 'et__min_samples_split': 5}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_rf_cvec.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Tress Gridsearch Training Score: 0.7866369710467706\n",
      "Extra Tress Gridsearch Testing Score: 0.7682030728122913\n"
     ]
    }
   ],
   "source": [
    "print(f'Extra Tress Gridsearch Training Score: {grid_rf_cvec.score(X_train, y_train)}')\n",
    "print(f'Extra Tress Gridsearch Testing Score: {grid_rf_cvec.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_poor accuracy but not too overfit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
